{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "\n",
    "## SMAI ASSIGNMENT-1\n",
    "</center>\n",
    "\n",
    "#### SUBMITTED BY: AISHWARYA SHIVACHANDRA\n",
    "#### ROLL NO. : 2018202005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Train decision tree only on categorical data. Report precision, recall, f1 score and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Algorithm implemented:</u>\n",
    "Firstly, split the given dataset using 80-20% train-validation split.\n",
    "Then, from the given dataset, considering only categorical data and taking attribute <b>'left'</b> as target attribute:\n",
    "<br>calculated entropy for dataset and each of the attributes\n",
    "<br>and used this information to obtain maximum information gain and start building the decision tree.\n",
    "\n",
    "<u> Results and observations:</u>\n",
    "<br>Precision: 1.0 \n",
    "<br>Recall: 0.001834862\n",
    "<br>F1-score: 0.003663003\n",
    "<br>Accuracy: 0.758007117438\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train the decision tree with categorical and numerical features. Report precision, recall, f1 score and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Algorithm implemented:</u>\n",
    "Firstly, split the given dataset using 80-20% train-validation split.\n",
    "To train the decision tree, numerical as well as categorical data has been used.\n",
    "Then, from the given dataset, considering all attributes and taking <b>'left'</b> as target attribute:\n",
    "<br>calculated entropy for dataset and each of the attributes.\n",
    "<br> for numerical attributes, obtained the split point which gives minimum entropy.\n",
    "<br>Then used this information to obtain maximum information gain and used this node with max Information Gain as the node for building the decision tree.\n",
    "\n",
    "\n",
    "<u> Results and observations:</u>\n",
    "<br>Precision:  0.950639853748\n",
    "<br>Recall:  0.954128440367\n",
    "<br>F1-score:  0.952380952381\n",
    "<br>accuracy:  0.976868327402"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Contrast the effectiveness of Misclassification rate, Gini, Entropy as impurity measures in terms of precision, recall and accuracy\n",
    "<u>Algorithm implemented:</u>\n",
    "Using the approach used in Q2 above, we again build decision tree to calculate precision, recall and accuracy.\n",
    "Also, find Maximum information gain and thus build decision tree using Gini Index and Missclassification rate respectively to see the contrast between the three methods.\n",
    "\n",
    "<u> Results and observations:</u><br>\n",
    "=>Using Gini Index as impurity measure to find Information gain and build tree, we found:\n",
    "<br>Precision:  0.972709551657\n",
    "<br>Recall:  0.915596330275\n",
    "<br>F1:  0.943289224953\n",
    "<br>Accuracy:  0.973309608541\n",
    "\n",
    "=>Using Entropy as impurity measure to find Information gain and build tree, we found:\n",
    "<br>Precision:  0.950639853748\n",
    "<br>Recall:  0.954128440367\n",
    "<br>F1:  0.952380952381\n",
    "<br>Accuracy:  0.976868327402\n",
    "\n",
    "=>Using Misclassification as impurity measure to find Information gain and build tree, we found:\n",
    "<br>Precision:  0.950099800399\n",
    "<br>Recall:  0.873394495413\n",
    "<br>F1:  0.910133843212\n",
    "<br>accuracy:  0.958185053381"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualise training data on a 2-dimensional plot taking the two features with maximum information gain\n",
    "\n",
    "Calculate Information Gain of every attribute to obtain those two attributes which have highest Information Gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://i.imgur.com/aJYcl09.png\" width=\"760px\" height=\"760px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotted a Graph to visualise scattering of dataset using 2 attributes with maximum information gain:\n",
    "<br>1)satisfaction_level on X-axis \n",
    "<br>2)number_project on Y-axis to visualise decision tree boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://i.imgur.com/3pLajOD.png\" width=\"500px\" height=\"500px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plot a graph of training and validation error with respect to to number of nodes in the decision tree.\n",
    "<u>Algorithm implemented:</u>\n",
    "<br>\n",
    "Starting from root of tree, calculate accuracy and thus error for the following cases:<br>\n",
    "1) for validation data,<br>\n",
    "2) for training data<br>\n",
    "for each of the above datasets, vary the number of nodes from 500 till length of the respective datasets and calculate the error for each and plot scatter graph as shown.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://i.imgur.com/r1VHGPu.png\" width=\"500px\" height=\"500px\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u> Results and observations:</u>\n",
    "<br>\n",
    "As we increase the number of nodes to traverse, the error increases.\n",
    "<br>This happens because when the number of nodes to be traversed is large, the dataset is less splitted and thus accuracy is very low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explain how decision tree is suitable to handle missing values in data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When some attributes are missing in test sample dataset, then we will consider all the possible values which target attribute can take from\n",
    "original dataset and turn-wise predict result for each of the possible value. Then from the obtained result whichever value has the highest\n",
    "probability will be returned as the predicted value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div><img src = \"https://i.imgur.com/94VWbd2.jpg\" width=\"500px\" height=\"500px\" label=\"decision tree\"></img><label>fig:- decision tree for example 1</label></div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>Example 1:<br></u>\n",
    "in dataset if given attributes are: <b>[A,B,C,D]</b>.\n",
    "At each leaf node, we calculate the number of positive results for target attribute as well as number of negative results for the same as shown in the figure as n1,n2,n3 and so on.\n",
    "\n",
    "Given missing-value test-case is <b>[a1,__, c1, __]</b>\n",
    "Since the value for \"B\" attribute is missing, we sum up the result for all possible cases of B,(b1 and b2).\n",
    "At the next step, we have the value for \"C\" in the given test dataset so we will consider n1 and n2.\n",
    "Since the value for \"D\" attribute is missing, we consider all n5,n6,n7,n8.\n",
    "\n",
    "Finally, if positive results for target attribute are more than the negative results, then we return \"TRUE\" as predicted value, otherwise return \"FALSE\".\n",
    "\n",
    "<br><u>\n",
    "Example 2:<br></u>\n",
    "in dataset if given attributes are:<b> [Outlook, Temperature, Humidity, Wind, Play]</b> and Play is the attribute for which value has to be predicted.\n",
    "Given missing-value test-case is :<b>[rain, hot, high,__]</b>\n",
    " \n",
    "Here, value for \"Wind\" Attribute is missing and \"Wind\" can take values<b> [\"weak\", \"normal\", \"strong\"]</b>\n",
    "from Given Dataset.\n",
    "Then we'll find result for all the three possible cases i.e.,<b>[overcast, hot, high, weak], [overcast, hot, high, normal],  [overcast, hot, high, strong].</b>\n",
    "\n",
    "Finally, if positive results for target attribute are more than the negative results, then we return \"TRUE\" as predicted value, otherwise return \"FALSE\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
